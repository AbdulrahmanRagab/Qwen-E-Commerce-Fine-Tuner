<p align="center">
  <img src="https://img.shields.io/badge/Qwen1.5--0.5B-Fine--Tuned-blueviolet?style=for-the-badge&logo=huggingface" />
  <img src="https://img.shields.io/badge/LoRA-PEFT-orange?style=for-the-badge" />
  <img src="https://img.shields.io/badge/Platform-Google%20Colab-yellow?style=for-the-badge&logo=googlecolab" />
  <img src="https://img.shields.io/badge/License-MIT-green?style=for-the-badge" />
</p>

# ğŸ›’ Fine-Tuning Qwen1.5-0.5B-Chat for Amazon Product Content Generation

> **Generate realistic product names and descriptions from just a category label** â€” powered by a LoRA fine-tuned Qwen1.5-0.5B-Chat model trained on real Amazon product data.

---

## ğŸ“‘ Table of Contents

- [ğŸ¤– Overview](#-overview)
- [âœ¨ Features](#-features)
- [ğŸ§° Tech Stack](#-tech-stack)
- [ğŸ§  Architecture](#-architecture)
- [ğŸ” Training Pipeline Flow](#-training-pipeline-flow)
- [ğŸ“š Dataset & Data Processing](#-dataset--data-processing)
- [ğŸ¥ Demo](#-demo)
- [ğŸš€ Getting Started](#-getting-started)
  - [âœ… Prerequisites](#-prerequisites)
  - [ğŸ“¦ Installation](#-installation)
  - [â–¶ï¸ Run the App](#ï¸-run-the-app)
- [âš™ï¸ Configuration](#ï¸-configuration)
- [ğŸ” How It Works (Step-by-Step)](#-how-it-works-step-by-step)
- [ğŸ› ï¸ Customization](#ï¸-customization)
- [ğŸ§¯ Troubleshooting](#-troubleshooting)
- [âš ï¸ Known Limitations](#ï¸-known-limitations)
- [ğŸ” Security Notes](#-security-notes)
- [ğŸ—ºï¸ Roadmap Ideas](#ï¸-roadmap-ideas)
- [ğŸ™ Acknowledgements / Sources](#-acknowledgements--sources)
- [ğŸ“„ License](#-license)
- [ğŸ“ Project Structure](#-project-structure)

---

## ğŸ¤– Overview

This project fine-tunes **Qwen1.5-0.5B-Chat** â€” a lightweight yet capable causal language model â€” using **LoRA (Low-Rank Adaptation)** on a real-world Amazon product dataset. Given only a **product category** (e.g., `Smartphones`, `BatteryChargers`), the model learns to generate:

| Task Type              | Example Input        | Example Output                                          |
|------------------------|----------------------|---------------------------------------------------------|
| **Product Name**       | `Smartphones`        | `Samsung Galaxy M14 5G (Berry Blue, 6GB, 128GB)`        |
| **Product Description**| `WirelessEarbuds`    | `Immersive sound with deep bass, 24h battery life...`   |

### Why This Matters

- **E-commerce Automation**: Auto-generate catalog content for thousands of products.
- **Content at Scale**: Marketing teams can bootstrap product listings from category labels alone.
- **Efficient Fine-Tuning**: LoRA trains only **0.74%** of total parameters â€” enabling fine-tuning on free-tier Google Colab GPUs.

---

## âœ¨ Features

| Feature | Description |
|---------|-------------|
| ğŸ”§ **LoRA Fine-Tuning** | Parameter-efficient fine-tuning â€” only 1.8M trainable params out of 464M |
| ğŸ“Š **Comprehensive Evaluation** | ROUGE-1/2/L, BLEU-1/2/3/4, METEOR, BERTScore, Perplexity |
| ğŸ§ª **Dual Notebook Workflow** | Separate notebooks for training and evaluation â€” clean separation of concerns |
| ğŸ“ˆ **Training Visualization** | Real-time loss curves for both training and evaluation |
| ğŸ”€ **Dual Task Support** | Single model handles both Product Name and Product Description generation |
| ğŸ’¾ **Model Merging** | LoRA adapters merged back into base model for standalone deployment |
| ğŸ†“ **Free-Tier Friendly** | Designed to run on Google Colab free tier (T4 GPU) |
| ğŸ“‹ **Gap Analysis** | Automatic train vs. test performance comparison to detect overfitting |

---

## ğŸ§° Tech Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Base Model** | [Qwen1.5-0.5B-Chat](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat) | Pre-trained causal language model |
| **Fine-Tuning** | [PEFT (LoRA)](https://github.com/huggingface/peft) | Parameter-efficient adapter training |
| **Training** | [HuggingFace Transformers](https://github.com/huggingface/transformers) | Trainer API, tokenization, generation |
| **Optimization** | [BitsAndBytes](https://github.com/TimDettmers/bitsandbytes) | 8-bit paged AdamW optimizer |
| **Acceleration** | [HuggingFace Accelerate](https://github.com/huggingface/accelerate) | Mixed-precision and distributed setup |
| **Data** | [HuggingFace Datasets](https://github.com/huggingface/datasets) | Dataset loading, splitting, mapping |
| **Evaluation** | `rouge-score`, `nltk`, `bert-score` | Multi-metric generation quality assessment |
| **Environment** | Google Colab + Google Drive | Free GPU compute + persistent storage |
| **Language** | Python 3.10+ | Core programming language |

---

## ğŸ§  Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ARCHITECTURE OVERVIEW                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚  Amazon CSV   â”‚â”€â”€â”€â–¶â”‚  Data Processor  â”‚â”€â”€â”€â–¶â”‚  HF Dataset  â”‚  â”‚
â”‚   â”‚  (Raw Data)   â”‚    â”‚  (Clean/Format)  â”‚    â”‚ (Train/Test) â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                       â”‚          â”‚
â”‚                                                       â–¼          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚  Qwen1.5     â”‚â”€â”€â”€â–¶â”‚   LoRA Adapter   â”‚â”€â”€â”€â–¶â”‚   Trainer    â”‚  â”‚
â”‚   â”‚  0.5B-Chat   â”‚    â”‚  (r=8, Î±=16)     â”‚    â”‚  (500 steps) â”‚  â”‚
â”‚   â”‚  (Frozen)    â”‚    â”‚  q/k/v/o_proj     â”‚    â”‚              â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                       â”‚          â”‚
â”‚                                                       â–¼          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚  Merged      â”‚â—€â”€â”€â”‚   Merge Weights   â”‚â—€â”€â”€â”‚  LoRA Saved  â”‚  â”‚
â”‚   â”‚  Model       â”‚    â”‚  (merge_and_     â”‚    â”‚  Adapter     â”‚  â”‚
â”‚   â”‚  (Deploy)    â”‚    â”‚   unload)         â”‚    â”‚  Checkpoint  â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚          â”‚                                                       â”‚
â”‚          â–¼                                                       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚   â”‚         EVALUATION SUITE             â”‚                      â”‚
â”‚   â”‚  ROUGE â”‚ BLEU â”‚ METEOR â”‚ BERTScore  â”‚                      â”‚
â”‚   â”‚              Perplexity              â”‚                      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### LoRA Configuration

| Parameter | Value | Description |
|-----------|-------|-------------|
| `r` | 8 | Rank of the low-rank decomposition |
| `lora_alpha` | 16 | Scaling factor (effective LR = Î±/r = 2) |
| `target_modules` | `q_proj`, `k_proj`, `v_proj`, `o_proj` | Attention projection layers |
| `lora_dropout` | 0.05 | Dropout on LoRA layers |
| `bias` | `none` | No bias terms trained |
| `task_type` | `CAUSAL_LM` | Causal language modeling objective |

> **Trainable Parameters**: 1,769,472 / 463,583,232 â‰ˆ **0.38%** of total model

---

## ğŸ” Training Pipeline Flow

```
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Load    â”‚â”€â”€â”€â”€â–¶â”‚  Prepare â”‚â”€â”€â”€â”€â–¶â”‚  Apply   â”‚â”€â”€â”€â”€â–¶â”‚  Train   â”‚
   â”‚  CSV     â”‚     â”‚  Dataset â”‚     â”‚  LoRA    â”‚     â”‚  Model   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                                                           â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Save   â”‚â”€â”€â”€â”€â–¶â”‚  Merge   â”‚â”€â”€â”€â”€â–¶â”‚  Test    â”‚â”€â”€â”€â”€â–¶â”‚ Evaluate â”‚
   â”‚  LoRA   â”‚     â”‚  Weights â”‚     â”‚  Model   â”‚     â”‚ Metrics  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Detailed Steps

| Step | Notebook | Action |
|------|----------|--------|
| 1 | `Fine_Tuning_Qwen.ipynb` | Load & preprocess Amazon product CSV |
| 2 | `Fine_Tuning_Qwen.ipynb` | Create prompt-formatted HuggingFace Dataset |
| 3 | `Fine_Tuning_Qwen.ipynb` | Load Qwen1.5-0.5B-Chat + tokenizer |
| 4 | `Fine_Tuning_Qwen.ipynb` | Attach LoRA adapters to attention layers |
| 5 | `Fine_Tuning_Qwen.ipynb` | Train for 500 steps (eval every 25 steps) |
| 6 | `Fine_Tuning_Qwen.ipynb` | Save LoRA adapter + merge into base model |
| 7 | `Fine_Tuning_Qwen.ipynb` | Quick ROUGE evaluation + sample generations |
| 8 | `Test_Finetuned_Model.ipynb` | Load merged model from Drive |
| 9 | `Test_Finetuned_Model.ipynb` | Generate predictions on 20 test + 10 train samples |
| 10 | `Test_Finetuned_Model.ipynb` | Compute ROUGE, BLEU, METEOR, BERTScore, Perplexity |
| 11 | `Test_Finetuned_Model.ipynb` | Generate summary report with gap analysis |

---

## ğŸ“š Dataset & Data Processing

### Source Data

| Property | Details |
|----------|---------|
| **File** | `amazon_product_details.csv` |
| **Source** | Amazon product listings |
| **Key Columns** | `category`, `product_name`, `about_product` |
| **Split** | 75% Train / 25% Test (shuffled, seed=0) |

### Data Transformation

The raw CSV is transformed into two task-oriented datasets that are then combined:

```
Raw CSV Record:
  category: "Electronics|Mobiles|Smartphones"
  product_name: "Samsung Galaxy M14 5G..."
  about_product: "Immersive 16.72cm display..."

        â†“  Split into TWO records  â†“

Record 1 (Product Name Task):
  category: "Smartphones"           â† Last segment of pipe-delimited category
  task_type: "Product Name"
  text: "Samsung Galaxy M14 5G..."

Record 2 (Product Description Task):
  category: "Smartphones"
  task_type: "Product Description"
  text: "Immersive 16.72cm display..."
```

### Prompt Template

```
Given the product category, you need to generate a '{task_type}'.
### Category: {category}
### {task_type}: {text}
```

### Tokenization

| Parameter | Value |
|-----------|-------|
| `max_length` | 400 tokens |
| `padding` | `max_length` (left-padded) |
| `truncation` | `True` |
| `EOS token` | Appended to all inputs |

---

## ğŸ¥ Demo

### Training Loss Curves

The training process produces both training and evaluation loss curves logged every 25 steps:

```
Step  25  â”‚ Train Loss: ~3.2  â”‚ Eval Loss: ~3.0
Step 100  â”‚ Train Loss: ~2.5  â”‚ Eval Loss: ~2.4
Step 250  â”‚ Train Loss: ~1.8  â”‚ Eval Loss: ~1.9
Step 500  â”‚ Train Loss: ~1.5  â”‚ Eval Loss: ~1.6
```

### Adjust LoRA for More/Less Capacity

```python
# More capacity (slower training, better results)
config = LoraConfig(r=16, lora_alpha=32, ...)

# Less capacity (faster training, possibly worse results)
config = LoraConfig(r=4, lora_alpha=8, ...)

# Target more layers
target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
```

### Train Longer or Shorter

```python
# In TrainingArguments
max_steps=1000,         # Longer training
learning_rate=1e-5,     # Lower LR for longer training
save_steps=50,          # Less frequent saves
```

### Add More Task Types

```python
# Example: Add "Product Tagline" as a new task
tagline = df[['category', 'tagline']].rename(columns={'tagline': 'text'})
tagline['task_type'] = 'Product Tagline'
df = pd.concat([products, description, tagline], ignore_index=True)
```

---

## ğŸ§¯ Troubleshooting

| Issue | Cause | Solution |
|-------|-------|----------|
| `OutOfMemoryError` | GPU VRAM exhausted | Reduce `per_device_train_batch_size` to 1, or use `gradient_accumulation_steps=4` |
| `CUDA out of memory` during eval | Large generation batch | Reduce `max_new_tokens` or evaluate fewer samples |
| Model outputs gibberish | Undertrained | Increase `max_steps` to 1000+ or lower `learning_rate` |
| Model repeats itself | Repetition penalty too low | Increase `repetition_penalty` to 1.3+ |
| `FileNotFoundError` for model | Wrong Drive path | Verify paths with `os.listdir()` â€” check notebook path cells |
| `tokenizer.pad_token is None` | Decoder-only LM quirk | Already handled: `tokenizer.pad_token = tokenizer.eos_token` |
| Very high perplexity (>1000) | Degenerate outputs | Filter outliers (already handled in code: `if ppl < 10000`) |
| Training loss doesn't decrease | Learning rate too low/high | Try `learning_rate` in range `[1e-5, 5e-5]` |
| Google Drive disconnects | Colab timeout | Use `Colab Pro` or save checkpoints frequently (`save_steps=25`) |
| `ImportError` for packages | Package not installed | Run the `pip install` cells at the top first |

---

## âš ï¸ Known Limitations

| Limitation | Details |
|------------|---------|
| **Small Base Model** | Qwen1.5-0.5B is a compact model â€” generation quality is limited compared to 7B+ models |
| **Low ROUGE/BLEU** | The model paraphrases rather than copying reference text â€” expected for generative models |
| **English Only** | Training data and evaluation are English-only |
| **Category Dependency** | Performance varies by category â€” categories with more training samples produce better results |
| **No Hallucination Control** | The model may generate plausible-sounding but factually incorrect product details |
| **Single Dataset** | Trained only on Amazon product data â€” may not generalize to other e-commerce platforms |
| **No Quantization at Inference** | Merged model runs in FP16 â€” could be further optimized with GPTQ/AWQ |
| **Colab Dependency** | Designed for Google Colab â€” running locally requires path adjustments |
| **Token Limit** | Max 400 tokens â€” very long product descriptions may be truncated |

---

## ğŸ” Security Notes

| âš ï¸ Security Consideration | Recommendation |
|---------------------------|----------------|
| **Google Drive Paths** | Hardcoded paths are exposed in notebooks â€” avoid committing notebooks with sensitive paths |
| **API Keys** | No API keys are used in this project â€” all models are loaded locally |
| **Model Outputs** | Generated content should be reviewed before publishing â€” models can produce misleading text |
| **Dataset Privacy** | The Amazon product dataset is publicly available â€” but verify licensing before commercial use |
| **Model Weights** | If sharing the fine-tuned model, ensure compliance with Qwen's model license (Apache 2.0) |
| **Colab Sessions** | Colab sessions may leave model weights in temporary storage â€” clear `/content/` after use |

---

## ğŸ—ºï¸ Roadmap Ideas

- [ ] ğŸ”„ **Quantize merged model** with GPTQ/AWQ for faster inference
- [ ] ğŸŒ **Build a Gradio/Streamlit UI** for interactive product content generation
- [ ] ğŸ“Š **Scale training data** with more Amazon categories and products
- [ ] ğŸ§ª **Experiment with larger models** (Qwen1.5-1.8B, Qwen2-7B)
- [ ] ğŸ·ï¸ **Add more task types**: product taglines, bullet points, SEO keywords
- [ ] ğŸ” **Implement DPO/RLHF** for preference-aligned generation
- [ ] ğŸ“ˆ **Add W&B / MLflow logging** for experiment tracking
- [ ] ğŸ³ **Dockerize** the inference pipeline for deployment
- [ ] ğŸŒ **Multi-language support** â€” fine-tune on multilingual product data
- [ ] ğŸ“¦ **Push to HuggingFace Hub** â€” share the fine-tuned adapter publicly
- [ ] âš¡ **vLLM / TGI integration** for production-grade serving
- [ ] ğŸ§ª **A/B testing framework** to compare generations from different checkpoints

---

## ğŸ™ Acknowledgements / Sources

| Resource | Credit |
|----------|--------|
| **Qwen1.5-0.5B-Chat** | [Alibaba Cloud / Qwen Team](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat) |
| **LoRA Paper** | [Hu et al., 2021 â€” "LoRA: Low-Rank Adaptation of Large Language Models"](https://arxiv.org/abs/2106.09685) |
| **PEFT Library** | [HuggingFace PEFT](https://github.com/huggingface/peft) |
| **HuggingFace Transformers** | [HuggingFace](https://github.com/huggingface/transformers) |
| **BERTScore** | [Zhang et al., 2020](https://arxiv.org/abs/1904.09675) |
| **Amazon Product Dataset** | Publicly available Amazon product listings |
| **Google Colab** | Free GPU compute platform by Google |
| **BitsAndBytes** | [Tim Dettmers](https://github.com/TimDettmers/bitsandbytes) |

---

## ğŸ“„ License

This project is licensed under the **MIT License** â€” see the [LICENSE](LICENSE) file for details.

The base model (Qwen1.5-0.5B-Chat) is licensed under **Apache 2.0** by Alibaba Cloud.

---

## ğŸ“ Project Structure

```
Fine-Tuning-Qwen/
â”‚
â”œâ”€â”€ ğŸ““ Fine_Tuning_Qwen.ipynb          # Main training notebook
â”‚   â”œâ”€â”€ Data loading & preprocessing
â”‚   â”œâ”€â”€ Model & tokenizer setup
â”‚   â”œâ”€â”€ LoRA configuration & attachment
â”‚   â”œâ”€â”€ Training (500 steps)
â”‚   â”œâ”€â”€ Loss visualization
â”‚   â”œâ”€â”€ LoRA adapter saving
â”‚   â”œâ”€â”€ Model merging (LoRA â†’ base)
â”‚   â”œâ”€â”€ Quick ROUGE evaluation
â”‚   â””â”€â”€ Sample generation tests
â”‚
â”œâ”€â”€ ğŸ““ Test_Finetuned_Model.ipynb       # Comprehensive evaluation notebook
â”‚   â”œâ”€â”€ Merged model loading
â”‚   â”œâ”€â”€ Dataset reconstruction
â”‚   â”œâ”€â”€ Batch prediction generation
â”‚   â”œâ”€â”€ ROUGE-1/2/L calculation
â”‚   â”œâ”€â”€ BLEU-1/2/3/4 calculation
â”‚   â”œâ”€â”€ METEOR calculation
â”‚   â”œâ”€â”€ BERTScore calculation
â”‚   â”œâ”€â”€ Perplexity calculation
â”‚   â”œâ”€â”€ Metrics summary table
â”‚   â””â”€â”€ Gap analysis (train vs test)
â”‚
â”œâ”€â”€ ğŸ“Š amazon_product_details.csv       # Source dataset
â”‚
â”œâ”€â”€ ğŸ“„ README.md                        # This file
â”‚
â”œâ”€â”€ ğŸ“„ LICENSE                          # MIT License
â”‚
â””â”€â”€ ğŸ“ (Generated on Google Drive)
    â”œâ”€â”€ ğŸ“ train-dir/                   # Training checkpoints
    â”‚   â”œâ”€â”€ checkpoint-25/
    â”‚   â”œâ”€â”€ checkpoint-50/
    â”‚   â”œâ”€â”€ ...
    â”‚   â””â”€â”€ logs/                       # TensorBoard logs
    â”‚
    â”œâ”€â”€ ğŸ“ qwen-lora-adapter/          # Saved LoRA adapter weights
    â”‚   â”œâ”€â”€ adapter_config.json
    â”‚   â”œâ”€â”€ adapter_model.safetensors
    â”‚   â”œâ”€â”€ tokenizer.json
    â”‚   â””â”€â”€ ...
    â”‚
    â””â”€â”€ ğŸ“ qwen-merged/                # Final merged model (ready for deployment)
        â”œâ”€â”€ config.json
        â”œâ”€â”€ model.safetensors
        â”œâ”€â”€ tokenizer.json
        â”œâ”€â”€ tokenizer_config.json
        â””â”€â”€ ...
```

---

<p align="center">
  <b>â­ If this project helped you, consider giving it a star! â­</b>
</p>

<p align="center">
  Made with â¤ï¸ using HuggingFace ğŸ¤— + LoRA + Qwen
</p>
